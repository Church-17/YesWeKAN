{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto DL24: YesWeKAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚≠êNel notebook **Introduzione KAN** abbiamo spiegato nel dettaglio le idee alla base del nostro progetto, le motivazioni che hanno guidato la nostra scelta e abbiamo introdotto l'argomento con un aporoccio divulgativo e volto alla comprensione dell'argomento da parte di chi si affacci per la prima volta alla scoperta delle reti KAN.\n",
    "\n",
    "Con questo notebook ci proponiamo invece di presentare il cuore del nostro lavoro che parte dalle operazioni di preprocessing e trattamento dei dati nel dataset e arriva alla trasparenza algoritmica passando per la definizione delle architetture d'interesse per il lavoro e il loro confronto prima in termini prestazionali e poi sotto l'aspetto dell'equit√† algoritmica.\n",
    "\n",
    "Tutti i modelli presentati in questo notebook sono stati definiti e ottimizzati mediante delle operazioni di tuning eseguite e descritte nel notebook **Tuning degli iperparametri**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importazione librerie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìöCome operazione preliminare importiamo le **librerie** che saranno utilizzate in questo notebook, per chiarezza le abbiamo divise in gruppi:\n",
    "\n",
    "- Librerie di **sistema**: intriducono funzionalit√† a basso-medio livello\n",
    "- Librerie per l'utilizzo di **Matrici, dataframe e tensori**\n",
    "- Librerie per l'utilizzo di **Modelli di Machine Learning e Deep Learning**\n",
    "- Librerie **grafiche**: introducono funzioni per la stampa di tabelle, grafici e strumenti di visualizzazione dei risultati\n",
    "- Librerie **custom**: Introducono i livelli DenseKAN e di funzioni di utilit√†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerie di sistema\n",
    "import os\n",
    "import random\n",
    "import base64\n",
    "import datetime\n",
    "\n",
    "# Array, dataframe e tensori\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Modelli ML & DL\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.utils import check_random_state\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Librerie grafiche\n",
    "from facets_overview.feature_statistics_generator import FeatureStatisticsGenerator  # strumento di visualizzazione per l‚Äôesplorazione dei dati\n",
    "from IPython.core.display import HTML  # funzioni utilizzate per visualizzare output HTML nel notebook\n",
    "import matplotlib.pyplot as plt # fornisce funzioni per generare grafici\n",
    "import seaborn as sns\n",
    "import networkx as nx # fornisce funzioni per la creazione di grafi\n",
    "\n",
    "# Librerie custom\n",
    "import utility as ut\n",
    "from tfkan import DenseKAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è Per rendere l'esperimento ripetibile, andiamo ad *impostare il seme casuale* di tutte le librerie che fanno utilizzo della casualit√† nell'elaborazione, in modo che queste vadano a generare sequenzialmente gli stessi valori ad ogni riavvio del notebook (o meglio, del kernel Python che esegue il notebook). \n",
    "\n",
    "Ci teniamo a far notare che in questo modo, nel caso si esegua pi√π volte la stessa cella, essa probabilmente non presenter√† gli stessi risultati, andando a *compromettere la riproducibilit√†* di tutte le celle di codice a seguire. Per riportare il flusso di esecuzione ad uno stato di riproducibilit√† sar√† necessario *riavviare il kernel* Python del notebook. \n",
    "\n",
    "La scelta di impostare il seed e non ogni volta il random state √® dovuta al fatto che per alcune librerie risultava difficoltoso se non impossibile impostare ogni random state, oltre ad un fattore di comodit√† maggiore ed un fattore di rischio minore nella scrittura del codice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valore del seme causale\n",
    "seed_value = 0\n",
    "\n",
    "# Impostazione dei semi casuali per os, random, numpy e tensorflow\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.keras.utils.set_random_seed(seed_value)\n",
    "\n",
    "# Controllo che il seme sia stato correttamente impostato\n",
    "check_random_state(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scelta del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí± Visto che il **cambio radicale** nell'architettura avviene al **livello degli archi** tra input e output di un qualsiasi livello, √® possibile definire tutti i principali tipi di livelli visti durante il corso: non solo i livelli densamente connessi, ma anche convolutivi, ricorrenti, e perfino transformer. Tuttavia in questo progetto, dovendo implementare autonomamente questo nuovo tipo di rete, abbiamo optato per usare dei **livelli densamente connessi**.\n",
    "\n",
    "ü©∫ Per la scelta del dataset, abbiamo pensato di non focalizzarci sulle possibili applicazioni citate nel paper, ma di approfondire qualche altro settore. Vista la maggiore spiegabilit√† delle reti KAN rispetto ad una rete MLP, abbiamo scelto un dataset in **campo medico**, in cui la spiegabilit√† e la trasperanza algoritmica sono fattori determinanti non solo per il personale medico, che ha -anche- il compito di valutare le decisioni della rete, ma anche per i pazienti, in modo che possano prendere coscienza dei motivi dietro le scelte mediche.\n",
    "\n",
    "üè• Il dataset che abbiamo scelto, dopo aver guardato le opzioni disponibili su [Kaggle](https://www.kaggle.com/), √® [Hospital Length of Stay Dataset Microsoft](https://www.kaggle.com/datasets/aayushchou/hospital-length-of-stay-dataset-microsoft). Qui il task √® di **regressione**, e consiste nello stimare i giorni di ricovero di un paziente, conoscendo diversi dati personali e medici di quest'ultimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataset che abbiamo scelto per il nostro task contiene 100.000 records, ognuno dei quali √® relativo al ricovero ospedaliero di un paziente.\n",
    "Gli attributi considerati sono in parte relativi all'identit√† del paziente, e in parte alla sua condizione clinica e sanitaria. \n",
    "\n",
    "Come prima cosa ci salviamo il nome del dataset in una variabile, usiamo un impostazione di pandas che permette di cambiare il tipo degli attributi nel dataset e poi carichiamo il dataset in un dataframe pandas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variabili del dataset\n",
    "DATASET_NAME = 'LengthOfStay'\n",
    "FEATURES = [ # Nomi feature accorciati (per migliorare la visualizzazione nei grafici)\n",
    "    'vdate', 'rcount', 'gender', 'dialysis', 'asthma', 'irondef', 'pneum', 'subdep', 'psychmaj',\n",
    "    'depress', 'psychother', 'fibrosis', 'malnutr', 'hemo', 'hematocrit', 'neutrophils', 'sodium',\n",
    "    'glucose', 'bun', 'creatinine', 'bmi', 'pulse', 'respiration', 'secdiag', 'facid'\n",
    "]\n",
    "\n",
    "# Numero feature usate dal modello\n",
    "N_FEATURES = len(FEATURES)\n",
    "\n",
    "# Definisce i colori da usare per le barre\n",
    "COLORS = ['b', 'r', 'g', 'c', 'm', 'y', 'k']  \n",
    "\n",
    "# Consente che si possa cambiare il tipo degli attributi nel dataset\n",
    "pd.set_option('future.no_silent_downcasting', True)    \n",
    "\n",
    "# Carica il dataset in un Pandas Dataframe\n",
    "df = pd.read_csv(f\"datasets/{DATASET_NAME}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîéEseguiamo un analisi grafica del dataset osservando la distribuzione dei vari attributi, gli estremi ed eventuali valori mancanti nei record.\n",
    "\n",
    "Per farlo utilizziamo la classe `FeatureStatisticsGenerator` che produce un risultato in formato HTML dunque visualizzabile sul notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera statistiche sul dataset\n",
    "fsg = FeatureStatisticsGenerator()\n",
    "dataframes = [ {'table': df, 'name': DATASET_NAME}]\n",
    "censusProto = fsg.ProtoFromDataFrames(dataframes)\n",
    "protostr = base64.b64encode(censusProto.SerializeToString()).decode(\"utf-8\")\n",
    "\n",
    "# Impostazioni per la visualizzazione\n",
    "HTML_TEMPLATE = \"\"\"<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"></script> <link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html\"> <facets-overview id=\"elem\"></facets-overview> <script> document.querySelector(\"#elem\").protoInput = \"{protostr}\"; </script>\"\"\"\n",
    "html = HTML_TEMPLATE.format(protostr=protostr)\n",
    "\n",
    "# Mostra analisi grafica\n",
    "display(HTML(html)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osserviamo che non sono presenti valori mancanti in nessun attributo, ma sono presenti molti attributi sbilanciati nella distribuzione dei loro valori.\n",
    "\n",
    "Notiamo inoltre che anche i valori target seguono una distribuzione sbilanciata in cui sono molto pi√π frequenti i valori minori di quello medio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing del dataset\n",
    "\n",
    "üî¢Importando il dataset e visualizzando i record ci accorgiamo che sono presenti degli attributi categorici come date e stringhe, questi non possono essere elaborati direttamente da Tensorflow, infatti ricordiamo che le reti, essendo funzioni differenziabili, possono trattare esclusivamente tensori numerici. Dobbiamo dunque codificare tutti i valori in forma rigorosamente numerica.\n",
    "\n",
    "Gli attributi che devono essere ricodificati sono:\n",
    "\n",
    "- **eid**: √® un identificativo univoco del record nel dataset; non porta alcuna informazione, per cui non lo consideriamo.\n",
    "    \n",
    "- **vdate** e **discharged**: sono la data di inizio e fine del ricovero; si potrebbero codificare come interi da 0 a 365 (o 366 a seconda dell'anno), tuttavia per il nostro scopo √® opportuno eliminare la data di fine ricovero, in quanto non √® un'informazione che ci dovrebbe essere nota al momento della predizione.\n",
    "\n",
    "- **rcount**: √® il numero di ricoveri pregressi del paziente; trasformeremo il valore 5+ in 5.\n",
    "\n",
    "- **gender**: √® il genere del paziente; trasformeremo M in 0 ed F in 1.\n",
    "\n",
    "- **facid**: √® un identificativo del reparto ospedaliero; trasformeremo le lettere in numeri incrementali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rimuove la colonna \"discharged\"\n",
    "df.drop(\"discharged\", axis=1, inplace=True) \n",
    "\n",
    "# Rimuove la colonna \"eid\"\n",
    "df.drop(\"eid\", axis=1, inplace=True)\n",
    "\n",
    "# Codifica del 5+ in 5 in rcount\n",
    "keys = sorted(df[\"rcount\"].unique())\n",
    "values = range(len(keys))\n",
    "df.replace({\"rcount\": dict(zip(keys, values))}, inplace=True)\n",
    "\n",
    "# Codifica delle date come numeri interi progressivi\n",
    "keys = df[\"vdate\"].unique()\n",
    "values = [datetime.datetime.strptime(str(date), \"%m/%d/%Y\").timetuple().tm_yday for date in keys]\n",
    "df.replace({\"vdate\": dict(zip(keys, values))}, inplace=True)\n",
    "\n",
    "# Codifica del genere come intero binario\n",
    "keys = df[\"gender\"].unique()\n",
    "values = range(len(keys))\n",
    "df.replace({\"gender\": dict(zip(keys, values))}, inplace=True)\n",
    "\n",
    "# Codifica dei facid come numeri interi progressivi\n",
    "keys = df[\"facid\"].unique()\n",
    "values = range(len(keys))\n",
    "df.replace({\"facid\": dict(zip(keys, values))}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come sappiamo, le reti neurali necessitano che i tensori da esse elaborati contengano valori tra 0 e 1, dunque √® opportuno standardizzare i dati.\n",
    "Eseguiamo quindi una normalizzazione MinMax, che porter√† tutti i valori nell'intervallo [0, 1] e per farlo utilizzeremo la funzione `standardize`, definita nella nostra libreria custom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizzazione degli attributi del dataset\n",
    "df = df.astype(float)\n",
    "df.iloc[:, :-1] = ut.standardize(df.iloc[:, :-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separazione del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto il dataset ha solamente valori numerici. Separiamo il training set dal test set; in questo passaggio non introduciamo il set di validazione in quanto nelle celle immediatamente successive a queste predisporremo l'addestramento dei modelli con il metodo **k-fold**.\n",
    "\n",
    "Dopo esserci salvati in alcuni file csv le partizioni del dataset separate, convertiamo il dataframe in un tensore Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione rapporto di split\n",
    "TEST_RATIO = 0.20\n",
    "\n",
    "# Separazione train, test\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size=TEST_RATIO)\n",
    "\n",
    "# Creazione dei dataset di train, test in csv\n",
    "x_train.to_csv(\"datasets/x_train.csv\", index=False)\n",
    "y_train.to_csv(\"datasets/y_train.csv\", index=False)\n",
    "x_test.to_csv(\"datasets/x_test.csv\", index=False)\n",
    "y_test.to_csv(\"datasets/y_test.csv\", index=False)\n",
    "\n",
    "# Conversione del dataframe in tensore\n",
    "x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "x_test = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "\n",
    "# Stampa le dimensioni dei nuovi dataset creati\n",
    "print(\"train: \", x_train.shape,\" test: \",  x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold\n",
    "\n",
    "üóÇÔ∏èIl metodo di partizione **k-fold** consiste nel suddividere i dati del train set (quelli di cui disponiamo per l'addestramento) in $k$ partizioni di dimensione fissa e suddividere l'addestramento in altrettante fasi; durante ogni fase l'addestramento viene eseguito utilizzando $k - 1$ partizioni per il training e quella rimanente per la validazione.\n",
    "\n",
    "Questa pratica viene utilizzata per evitare che l'addestramento dei modelli sia troppo dipendente da una specifica partizione train-validation-test, infatti abbiamo notato che al variare di questa, anche le prestazioni subivano delle variazioni, talvolta piuttosto significative. Il k-fold conferisce all'addestramento robustezza rispetto alla variabilit√† introdotta dal partizionamento.\n",
    "\n",
    "Nel nostro caso utilizziamo $k = 4$ dunque 4 partizioni (o fold) perch√© riteniamo che questo possa essere un buon compromesso che assicura allo stesso tempo una buona variabilit√† nelle partizioni e tempistiche di addestramento accettabili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione degl indici di training e validation set\n",
    "kfold = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "train_val_indices = list(kfold.split(x_train))\n",
    "\n",
    "# Definizione delle liste di training e validation set\n",
    "x_train_fold_list = []\n",
    "y_train_fold_list = []\n",
    "x_val_fold_list = []\n",
    "y_val_fold_list = []\n",
    "for train_indices, val_indices in train_val_indices:\n",
    "    x_train_fold_list.append(tf.gather(x_train, train_indices))\n",
    "    x_val_fold_list.append(tf.gather(x_train, val_indices))\n",
    "    y_train_fold_list.append(tf.gather(y_train, train_indices))\n",
    "    y_val_fold_list.append(tf.gather(y_train, val_indices))\n",
    "\n",
    "training_validation_set = list(zip(x_train_fold_list, y_train_fold_list, x_val_fold_list, y_val_fold_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîßPer cercare di dare una risposta alla domanda che guida il nostro progetto, dobbiamo valutare se l'architettura KAN possa costituire una valida alternativa agli approcci di apprendimento allo stato dell'arte dalle reti fully connected agli algoritmi pi√π tradizionali.\n",
    "\n",
    "Per farlo dobbiamo confrontare diversi modelli sia in termini di prestazioni che in termini di equit√† algoritmica.\n",
    "\n",
    "Utilizzeremo le metriche tipiche di un task di regressione cio√® **MSE**, **MAE** e **RMSE**.\n",
    "\n",
    "Definiamo inoltre alcuni parametri relativi all'addestramento dei modelli che saranno utilizzati nei metodi `compile` e `fit`. Un particolare che risalta √® l'iperparametro **EPOCHS_PER_FOLD** che moltiplicato per il numero di fold $k$ √® uguale al numero totale di epoche di addestramento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista di metriche necessarie per valutare le performance \n",
    "METRICS_NAME = {\"mean_absolute_error\":\"mae\", \"mean_squared_error\":\"mse\", \"root_mean_squared_error\":\"rmse\", \"r2_score\": \"r2_score\"}\n",
    "METRICS = [tf.keras.metrics.get(metric_name) for metric_name in METRICS_NAME]\n",
    "for metric in METRICS:\n",
    "    metric.name = METRICS_NAME[metric.name]\n",
    "\n",
    "# Costanti dei modelli\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS_PER_FOLD = 75\n",
    "LOSS = \"mse\"\n",
    "\n",
    "# Lista dei modelli addestrati e delle loro performance\n",
    "elenco_modelli = {}\n",
    "histories = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La scelta di utilizzare il metodo k-fold ci pone la questione di come visualizzare l'andamento delle metriche e della funzione di loss con il progredire dell'addestramento sui diversi fold. Il metodo `fit` accetta infatti una sola partizione train-validation dunque dobbiamo eseguirlo $k$ volte e salvarci in un'unica lista tutto lo storico delle meriche che monitoriamo.\n",
    "\n",
    "La funzione `training` restituisce un dizionario che associa alle metriche del modello delle liste contenenti lo storico (history) dei loro valori durante l'addestramento.\n",
    "\n",
    "Per quanto riguarda l'addestramento dei modelli di ML tradizionale abbiamo scritto la funzione `trainingML` che semplicemente esegue l'addestramento su ciascun fold per un totale di $k$ iterazioni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per l'addestramento dei modelli DL\n",
    "def training(model) -> dict:\n",
    "    total_history = {}\n",
    "\n",
    "    # Training per ogni fold\n",
    "    for fold, (x_train_fold, y_train_fold, x_val_fold, y_val_fold) in enumerate(training_validation_set):\n",
    "        print(f\"Fold {fold}\")\n",
    "\n",
    "        history = model.fit(x_train_fold, y_train_fold, epochs=EPOCHS_PER_FOLD, batch_size=BATCH_SIZE, validation_data=(x_val_fold, y_val_fold), verbose=1)\n",
    "\n",
    "        # Memorizzazione della storia del training\n",
    "        for key, value in history.history.items():\n",
    "            if key not in total_history:\n",
    "                total_history[key] = value\n",
    "            else:\n",
    "                total_history[key] += value\n",
    "\n",
    "    return total_history\n",
    "\n",
    "# Funzione per l'addestramento dei modelli DL\n",
    "def trainingML(model):\n",
    "    for x_train_fold, y_train_fold, _, _ in training_validation_set:\n",
    "        model.fit(x_train_fold, y_train_fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definizione architettura KAN Classica\n",
    "üìúCome spiegato nell'articolo su cui ci basiamo e ribadito nel notebook **Tutorial**, ogni livello $l$ di una rete KAN contiene $inputs_l$ * $units_l$ funzioni spline, ognuna delle quali ha formula $$\\phi(x) = w_bb(x) + w_s\\sum{c_iB_i}$$\n",
    "\n",
    "Il numero totale dei parametri del livello si ottiene sommando:\n",
    "- I coefficienti $w_s$, uno per ogni spline; totale = $inputs_l$ * $units_l$\n",
    "- I coefficienti $c_i$, uno per ogni funzione basis, il cui numero per ogni spline √® uguale a $grid\\_size - spline\\_order - 1$; totale = $inputs_l$ * $units_l$ * $\\#B_i$\n",
    "- I bias $w_b$, uno per ogni spline; totale = $inputs_l$ * $units_l$\n",
    "\n",
    "Nonostante il conteggio dei parametri sembri maggiore rispetto a quello di un MLP vedremo che nel complesso ne serviranno molti meno per comparare e battere le prestazioni di quest'ultimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione del modello KAN con parametri del tuning\n",
    "kan = tf.keras.models.Sequential([\n",
    "    DenseKAN(16, grid_range=[-1, 1], grid_size=20, spline_order=2),\n",
    "    DenseKAN(4, grid_range=[-5, 5], grid_size=32, spline_order=3),\n",
    "    DenseKAN(8, grid_range=[-4.9, 4.9], grid_size=22, spline_order=3),\n",
    "    DenseKAN(1)\n",
    "])\n",
    "# Build del modello (chiamata necessaria a definire la dimensione dell'input quando si utilizza l'API Sequential)\n",
    "kan.build(input_shape=(None, N_FEATURES))\n",
    "kan.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.011774913804863748), loss=LOSS, metrics=METRICS)  # Compilazione del modello (iperparamentri ottimizzati in fase di tuning)\n",
    "\n",
    "# Stampa del sommario\n",
    "kan.summary()\n",
    "\n",
    "# Training modello e salvataggio performance\n",
    "histories['KAN'] = training(kan)\n",
    "elenco_modelli['KAN'] = kan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definizione architettura MLP Classica\n",
    "üß†A questo punto definiamo una classica **rete a livelli completamente connessi** MLP utilizzata molto frequentemente nelle architetture allo stato dell'arte.\n",
    "\n",
    "Come sempre utilizziamo delle funzioni di attivazione ReLU nei livelli intermedi mentre, trattandosi di un task di regressione, l'ultimo livello avr√† una sola unit√† di output e nessuna funzione di attivazione. Una particolarit√† da notare √® l'utilizzo di un regolarizzatore, in questo caso basato sulla **norma L2**: abbiamo notato che l'introduzione di questo fattore stabilizza l'addestramento limitandone fluttuazioni a cui normalmente si assiste con l'avanzamento delle epoche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione del modello MLP\n",
    "mlp = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(156, activation='elu', kernel_regularizer=tf.keras.regularizers.l2(0.00027151975248047394)),\n",
    "    tf.keras.layers.Dense(224, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0016076644854235774)),\n",
    "    tf.keras.layers.Dense(216, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0004701808479158671)),\n",
    "    tf.keras.layers.Dense(20, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.00020462523238833736)),\n",
    "    tf.keras.layers.Dense(12, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.000420656047032272)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "mlp.build(input_shape=(None, N_FEATURES))\n",
    "mlp.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0006224065081134311), loss=LOSS, metrics=METRICS)\n",
    "\n",
    "# Stampa sommario\n",
    "mlp.summary()\n",
    "\n",
    "# Training modello e salvataggio performance\n",
    "histories['MLP'] = training(mlp)\n",
    "elenco_modelli['MLP'] = mlp\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definizione architerrura KAN-MLP 1 (Livelli misti)\n",
    "üè≠Definiamo infine un'architettura **ibrida**, un mix tra KAN e MLP con l'idea di poter combinare in un'unica rete tutti i vantaggi di queste due.\n",
    "\n",
    "Da un lato desideriamo l'accuratezza, il minor numero di parametri e la scalabilit√† di una KAN, dall'altra cerchiamo di ridurre i tempi di addestramento avvicinandoli a quelli comparativamente minori di un MLP.\n",
    "\n",
    "Per quanto riguarda la spiegabilit√†, questa viene compromessa in maniera decisiva dalla presenza dei livelli Dense, infatti perdiamo completamente il controllo su quali valori entrano in input alle spline, e sebbene possiamo osservare la composizione delle funzioni, non sono spiegabili le trasformazioni dei valori in ingresso; per chiarire il concetto, sarebbe come se capissimo le operazioni che vengono fatte tra i dati, ma non sapessimo cosa quei dati rappresentano, quali elaborazioni hanno subito per arrivare alla forma che osserviamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione del modello KAN-MLP 1\n",
    "kanmlp = tf.keras.models.Sequential([\n",
    "    # DenseKAN(16),\n",
    "    # Dense(64),\n",
    "    # DenseKAN(6),\n",
    "    Dense(64),\n",
    "    DenseKAN(1)\n",
    "])\n",
    "kanmlp.build(input_shape=(None, N_FEATURES))\n",
    "kanmlp.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-3), loss=LOSS, metrics=METRICS)\n",
    "\n",
    "# Stampa sommario\n",
    "kanmlp.summary()\n",
    "\n",
    "# Training modello e salvataggio performance\n",
    "histories['KAN_MLP'] = training(kanmlp)\n",
    "elenco_modelli['KAN_MLP'] = kanmlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definizione architettura Albero Decisionale\n",
    "üå≤Introduciamo ora il primo modello basato sul Machine Learning tradizionale: un **albero decisionale di regressione**.\n",
    "\n",
    "Come abbiamo fatto per le reti ad apprendimento automatico, anche in questo caso abbiamo assegnato gli iperparametri in seguito ad un tuning.\n",
    "Uno dei motivi che ci ha spinto a optare per un albero √® la possibilit√† di stampare a video una rappresentazione del modello in modo da poter visualizzare gli attributi utilizzati in fase di predizione.\n",
    "\n",
    "Essendo l'albero un modello estremamente spiegabile, conoscere gli attributi che utilizza potrebbe darci una prima indicazione su quali siano i pi√π significativi in termini di discriminazione delle istanze.\n",
    "\n",
    "Tuttavia teniamo presente che i criteri utilizzati da un modello non danno nessuna indicazione su quelli utilizzati da un altro, in questo caso vogliamo pi√π che altro avere un'idea di massima sull'importanza relativa degli attributi.\n",
    "\n",
    "üîßAbbiamo impostato gli iperparametri del modello in seguito ad un tuning, useremo gli stessi in tutti gli alberi decisionali dei modelli successivi.\n",
    "\n",
    "Come metrica per valutare questo modello utilizzeremo il Coefficiente di determinazione $R^2$ definito come $$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} = 1 - \\frac{SSR}{SST}$$ in cui \n",
    "\n",
    "- $SSR = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ (Somma dei quadrati residui) quantifica la variabilit√† nelle predizioni del modello\n",
    "- $SST = \\sum_{i=1}^{n} (y_i - \\bar{y}_i)^2$ (Somma dei quadrati totali) quantifica la \"qualit√†\" nelle predizioni in  termini di scostamento dal valore medio, infatti un regressore che predice sistematicamente il valore medio non √® buono."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un albero decisionale per la regressione\n",
    "tree = DecisionTreeRegressor(\n",
    "    ccp_alpha=0.000000001,\n",
    "    criterion='squared_error',\n",
    "    max_depth=35,\n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    random_state=0,\n",
    "    splitter='best'\n",
    ")\n",
    "\n",
    "# Addestramento\n",
    "trainingML(tree)\n",
    "\n",
    "# Aggiunge il modello alla lista modelli\n",
    "elenco_modelli['TREE'] = tree\n",
    "\n",
    "# Stampa lo score R^2 del modello\n",
    "print(f\"R¬≤ score: {tree.score(x_test, y_test)}\")\n",
    "\n",
    "# Disegna l'albero\n",
    "plt.figure(figsize=(30,15))\n",
    "plot_tree(tree, max_depth=3, filled=True, rounded=True, feature_names=FEATURES)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definizione architettura Random Forest\n",
    "üå≤üå≤Come secondo approccio di Machine Learning tradizionale introduciamo una **Random Forest** di regressione; rispetto all'albero ci aspettiamo prestazioni migliori ma il prezzo da pagare (come spesso accade) sar√† una minore trasparenza.\n",
    "\n",
    "La foresta che utilizziamo √® composta da 250 alberi con gli stessi iperparametri utilizzati per l'albero precedentemente definito.\n",
    "\n",
    "Anche in questo caso visualizziamo il $R^2$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea il modello della random forest\n",
    "forest = RandomForestRegressor(\n",
    "    bootstrap=True,\n",
    "    ccp_alpha=0.0,\n",
    "    criterion='squared_error',\n",
    "    max_depth=35,\n",
    "    max_features=1.0,\n",
    "    max_leaf_nodes=None,\n",
    "    max_samples=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=5,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    n_estimators=250,\n",
    "    random_state=0,\n",
    "    verbose=0,\n",
    "    warm_start=False\n",
    ")\n",
    "\n",
    "# Addestramento\n",
    "trainingML(forest)\n",
    "\n",
    "# Aggiunge il modello alla lista modelli\n",
    "elenco_modelli['FOREST'] = forest\n",
    "\n",
    "# Stampa lo score R^2 del modello\n",
    "print(f\"R¬≤ score: {forest.score(x_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definizione architettura Ensemble\n",
    "üßë‚ÄçüîßInfine definiamo un modello di regressione **ensemble** composto da un albero decisionale, un Support Vector Regressor e un regressore bayesiano.\n",
    "\n",
    "Per questo modello abbiamo sperimentato anche delle altre configurazioni, in particolare, valutando l'opportunit√† di introdurre il SVR al posto di un molto pi√π semplice regressore lineare, abbiamo notato che a fronte di un notevole aumento delle tempistiche di addestramento, le metriche avevano miglioramenti intorno ai 10 punti percentuali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea il modello di ensemble\n",
    "ensemble = VotingRegressor(\n",
    "    estimators=[\n",
    "    ('tree', DecisionTreeRegressor(\n",
    "    ccp_alpha=0.000000001,\n",
    "    criterion='squared_error',\n",
    "    max_depth=35,\n",
    "    max_features=None,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    random_state=0,\n",
    "    splitter='best'\n",
    "    )),        \n",
    "    ('svm', SVR(kernel='rbf', C=10, epsilon=0.01, gamma=0.1, coef0=0.1, tol=0.001,cache_size=20)),\n",
    "    ('bayes', BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True, fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, tol=0.001, verbose=False))\n",
    "    ],\n",
    "    n_jobs=None,\n",
    "    verbose=False,\n",
    "    weights=None\n",
    ")\n",
    "\n",
    "# Addestramento\n",
    "trainingML(ensemble)\n",
    "\n",
    "# Aggiunge il modello alla lista modelli\n",
    "elenco_modelli['ENSEMBLE'] = ensemble\n",
    "\n",
    "print(f\"R¬≤ score: {ensemble.score(x_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confronto modelli\n",
    "A questo punto possiamo effettuare il confronto tra le **prestazioni** dei modelli appena definiti:\n",
    "- Preliminarmente calcoleremo le metriche di valutazione per ognuno dei modelli\n",
    "- Successivamente le rappresenteremo attraverso una **heatmap**, una visualizzazione molto efficiente che grazie al channel colore facilita il confronto e mette in risalto i modelli migliori e quelli peggiori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione funzione per calcolo metriche di errore del modello\n",
    "def calculate_metrics(model, X, y):\n",
    "    # Calcolo delle previsioni del modello\n",
    "    predictions = model.predict(X).reshape(-1)\n",
    "\n",
    "    # Calcolo metriche\n",
    "    result = {}\n",
    "    for metric in METRICS:\n",
    "        metric.reset_state()\n",
    "        result[metric.name] = float(metric(y, predictions))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {metric: [] for metric in list(METRICS_NAME.values())[:-1]}\n",
    "performance_r2 = {\"r2_score\": []}\n",
    "\n",
    "# Salvataggio performance di ogni modello\n",
    "for model in elenco_modelli.values():\n",
    "    # Calcolo delle metriche per il modello corrente\n",
    "    result_metrics = calculate_metrics(model, x_test, y_test)\n",
    "\n",
    "    # Aggiunta delle metriche al dizionario delle performance\n",
    "    for name, result in list(result_metrics.items())[:-1]:\n",
    "        performance[name].append(result)\n",
    "    performance_r2[\"r2_score\"].append(result_metrics[\"r2_score\"])\n",
    "\n",
    "# Creazione di un DataFrame con le performance dei modelli\n",
    "df_error = pd.DataFrame(performance, index=elenco_modelli.keys())\n",
    "df_r2 = pd.DataFrame(performance_r2, index=elenco_modelli.keys())\n",
    "\n",
    "# Visualizzazione della heatmap\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(df_error, annot=True, cmap='coolwarm', fmt='.6f')  \n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(df_r2, annot=True, cmap='coolwarm_r', fmt='.6f')  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nella seguente cella confrontiamo i nostri modelli sulla base della loro \"accuratezza\" di predizione; qui con accuratezza, con un piccolo abuso di linguaggio dalla terminologia della classificazione, intendiamo il confronto tra il numero di predizioni corrette e il numero di predizioni errate dei modelli.\n",
    "\n",
    "Trattandosi di un task di regressione si potrebbe anche introdurre un certo margine di tolleranza considerando predizioni corrette anche quelle che si discostano entro un certo range dal valore target (ad esempio, nel nostro caso, avremmo potuto considerare corrette predizioni con un giorno in pi√π o in meno di differenza dal ground truth).\n",
    "Abbiamo tuttavia preferito seguire un approccio pi√π rigido, anche considerando la delicatezza dell'ambito di applicazione, e prendere come corrette solo le predizioni che, a seguito dell'arrotondamento, sono identiche al target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione funzione per calcolo dell'errore delle predizioni\n",
    "def calculate_error_prediction(model, X, y):\n",
    "    rounded_predictions = np.round(model.predict(X).reshape(-1)).astype(int)\n",
    "    boolean_array_result = y.numpy() == rounded_predictions\n",
    "    count_elements = np.unique(boolean_array_result, return_counts=True)\n",
    "    count_elements = dict(np.transpose(count_elements))\n",
    "\n",
    "    return {'Predizioni sbagliate': count_elements[0], 'Predizioni corrette': count_elements[1]}\n",
    "\n",
    "# Inizializzazione del dizionario delle performance\n",
    "performance = {'Predizioni sbagliate': [], 'Predizioni corrette': []}\n",
    "\n",
    "for model in elenco_modelli.values():\n",
    "    # Calcolo delle metriche di errore per il modello corrente\n",
    "    result_predictions = calculate_error_prediction(model, x_test, y_test)\n",
    "\n",
    "    # Aggiunta delle metriche al dizionario delle performance\n",
    "    for name, result in result_predictions.items():\n",
    "        performance[name].append(result)\n",
    "\n",
    "# Creazione di un DataFrame con le performance dei modelli\n",
    "df_ = pd.DataFrame(performance, index=elenco_modelli.keys())\n",
    "df_err = pd.DataFrame(df_.iloc[:,0])\n",
    "df_corr = pd.DataFrame(df_.iloc[:,1])\n",
    "\n",
    "# Visualizzazione della heatmap\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.yticks([])\n",
    "plt.xticks([])\n",
    "plt.title(\"Heatmap delle performance dei modelli (Predizioni corrette, Predizioni sbagliate)\")\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(df_corr, annot=True, cmap='coolwarm_r', fmt='d')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(df_err, annot=True, cmap='coolwarm', fmt='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prestazione dei modelli di apprendimento automatico\n",
    "üìàEffettuiamo ora un confronto tra i modelli di apprendimento automatico e vediamo in che modo le metriche che stiamo monitorando vengono ottimizzate al passare delle epoche di addestramento.\n",
    "\n",
    "Abbiamo deciso di visualizzare i grafici utilizzando una **scala logaritmica** sull'asse verticale, questa infatti ci permette di evidenziare al meglio le fluttuazioni delle metriche dunque di farci un'idea grafica circa la stabilit√† dei modelli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione funzione di creazione linechart\n",
    "def plot_data(ax, epochs, metric, val_metric, model_name, metric_name, color):\n",
    "    ax.plot(epochs, metric, color, label=f'{model_name} Training ({metric_name})') \n",
    "    ax.plot(epochs, val_metric, color+'--', alpha=0.7, label=f'{model_name} Validation ({metric_name})') \n",
    "    ax.set_xlabel('Epoca')\n",
    "    ax.legend(fontsize='large')\n",
    "\n",
    "plt.figure(figsize=(20, 7))\n",
    "\n",
    "\n",
    "# Crea subplot per ogni metrica\n",
    "ax1 = plt.subplot(1, 4, 1)\n",
    "ax1.set_title('Perdita di addestramento')\n",
    "ax1.set_ylabel('MSE')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "ax2 = plt.subplot(1, 4, 2)\n",
    "ax2.set_title('Errore medio assoluto')\n",
    "ax2.set_ylabel('MAE')\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "ax3 = plt.subplot(1, 4, 3)\n",
    "ax3.set_title('Errore quadratico medio radicale')\n",
    "ax3.set_ylabel('RMSE')\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "ax4 = plt.subplot(1, 4, 4)\n",
    "ax4.set_title('R¬≤ score')\n",
    "ax4.set_ylabel('R¬≤')\n",
    "ax4.set_yscale('log')\n",
    "\n",
    "# Per ogni modello associato al suo colore\n",
    "for (model_name, history), color in zip(histories.items(), COLORS):\n",
    "\n",
    "    # Salvataggio performance\n",
    "    loss, val_loss, mae, val_mae, rmse, val_rmse, r2_score, val_r2_score = history['loss'], history['val_loss'], history['mae'], history['val_mae'], history['rmse'], history['val_rmse'], history['r2_score'], history['val_r2_score']\n",
    "\n",
    "    # Calcolo numero di epoche \n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    # Creazione 3 plot con performance di train e validation al variare delle epoche\n",
    "    plot_data(ax1, epochs, loss, val_loss, model_name, 'MSE', color)\n",
    "    plot_data(ax2, epochs, mae, val_mae, model_name, 'MAE', color)\n",
    "    plot_data(ax3, epochs, rmse, val_rmse, model_name, 'RMSE', color)\n",
    "    plot_data(ax4, epochs, r2_score, val_r2_score, model_name, 'R¬≤', color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi equit√† di genere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë´Dopo aver valutato i nostri modelli dal punto di vista prestazionale, ora considereremo l'aspetto dell'**equit√† algoritmica**.\n",
    "\n",
    "Confrontiamo i modelli sulle solite metriche, questa volta distinguendo i record in base all'attributo **gender** (che nel nostro caso assume due valori). \n",
    "\n",
    "I modelli pi√π equi (dunque preferibili sul piano dell'eqit√† algoritmica) avranno prestazioni simili per entrambe le categorie considerate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione delle categorie e sottogruppi\n",
    "CATEGORY = 'gender'\n",
    "INDEX_CATEGORY = FEATURES.index(CATEGORY)\n",
    "SUBGROUPS = df[CATEGORY].unique()\n",
    "SUBGROUP_NAMES = ['Maschio', 'Femmina']  # Aggiungiamo questa riga per i nomi leggibili\n",
    "\n",
    "# Creazione di un dizionario per memorizzare i risultati\n",
    "all_results = {}\n",
    "\n",
    "for model_name, model in elenco_modelli.items():\n",
    "    results = {}\n",
    "\n",
    "    for sg, sg_name in zip(SUBGROUPS, SUBGROUP_NAMES):\n",
    "        # Filtraggio del dataset di test per il sottogruppo corrente\n",
    "        subgroup_indices = x_test[:, INDEX_CATEGORY] == sg\n",
    "        features1 = x_test[subgroup_indices]\n",
    "        labels = y_test[subgroup_indices]\n",
    "        \n",
    "        # Calcolo delle metriche per ciascun sottogruppo\n",
    "        results[sg_name] = calculate_metrics(model, features1, labels)\n",
    "\n",
    "    all_results[model_name] = results\n",
    "\n",
    "# Creazione Dataframe performance per tutte le metriche tranne R2\n",
    "df_metrics = pd.DataFrame()\n",
    "\n",
    "# Creazione Dataframe performance per R2\n",
    "df_r2 = pd.DataFrame()\n",
    "\n",
    "for model_name, results in all_results.items():\n",
    "    # Definizione dataframe di un modello per tutte le metriche tranne R2\n",
    "    temp_df_metrics = pd.DataFrame({sg: [result[metric] for metric in METRICS_NAME.values() if metric != 'r2_score'] \n",
    "                                    for sg, result in results.items()}, \n",
    "                                   index=[metric for metric in METRICS_NAME.values() if metric != 'r2_score'])\n",
    "    temp_df_metrics.columns = [f'{model_name} - {sg}' for sg in SUBGROUP_NAMES]\n",
    "\n",
    "    # Definizione dataframe di un modello per R2\n",
    "    temp_df_r2 = pd.DataFrame({sg: [result['r2_score']] for sg, result in results.items()}, \n",
    "                              index=['r2_score'])\n",
    "    temp_df_r2.columns = [f'{model_name} - {sg}' for sg in SUBGROUP_NAMES]\n",
    "\n",
    "    # Concatenazione dataframe di un modello ai precedenti\n",
    "    df_metrics = pd.concat([df_metrics, temp_df_metrics], axis=1)\n",
    "    df_r2 = pd.concat([df_r2, temp_df_r2], axis=1)\n",
    "\n",
    "# Processamento dei dataset\n",
    "df_metrics = df_metrics.astype(float).transpose()\n",
    "df_r2 = df_r2.astype(float).transpose()\n",
    "\n",
    "# Visualizzazione delle heatmaps\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Heatmap per tutte le metriche tranne R2\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(df_metrics, annot=True, cmap='coolwarm', fmt=\".4f\")\n",
    "plt.title('Heatmap delle metriche di performance per genere per tutti i modelli')\n",
    "plt.ylabel('Modello - Genere')\n",
    "\n",
    "# Heatmap per R2\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(df_r2, annot=True, cmap='coolwarm_r', fmt=\".4f\")\n",
    "plt.title('Heatmap della metrica R2 per genere per tutti i modelli')\n",
    "plt.ylabel('Modello - Genere')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trasparenza algoritmica\n",
    "ü™üTra gli obiettivi principali del nostro progetto c'√® quello di valutare se e in che misura le KAN possano introdurre dei miglioramenti nella **trasparenza algoritmica**.\n",
    "\n",
    "Si tratta di un aspetto fondamentale in particolare su applicazioni che utilizzano dati sensibili come la nostra, per le quali non sarebbero accettabili elaborazioni di tipo black-box.\n",
    "\n",
    "Per studiare la spiegabilit√† abbiamo utilizzato tre metodi:\n",
    "\n",
    "- Feature importance\n",
    "- LIME (Local Interpretable Model-agnostic Explanations)\n",
    "- Grafo di rappresentazione dei livelli\n",
    "\n",
    "I primi due sono **agnostici** cio√® applicabili a qualsiasi modello, il terzo √® invece specifico per l'architettura delle KAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìäIl metodo di trasparenza algoritmica \"Feature Importance\" valuta l'importanza di ciascuna caratteristica di un dataset nel contribuire al modello predittivo. Generalmente, questa valutazione viene effettuata misurando l'impatto della rimozione o della permutazione di una caratteristica sulle performance del modello.\n",
    "\n",
    "\n",
    "Ad esempio, supponiamo che tu debba acquistare una nuova casa vicino al tuo posto di lavoro. Quando acquisti una casa, potresti pensare a diversi fattori. Il fattore pi√π importante nel tuo processo decisionale potrebbe essere l‚Äôubicazione della propriet√†, quindi probabilmente cercherai solo case vicine al posto che preferisci. Feature importance funziona in modo simile. Classificher√† le caratteristiche in base all‚Äôeffetto che hanno sulla previsione del modello.\n",
    "\n",
    "Feature importance √® importante per le seguenti ragioni:\n",
    "\n",
    "1. **Data Comprehension**:\n",
    "Aiuta anche a capire quali caratteristiche sono irrilevanti per il modello.\n",
    "\n",
    "2. **Model Improvement**:\n",
    "Durante l'addestramento del modello, √® possibile utilizzare i punteggi calcolati in base all'importanza delle funzionalit√† per ridurre la dimensionalit√† del modello. I punteggi pi√π alti vengono solitamente mantenuti e quelli pi√π bassi vengono cancellati poich√© non sono importanti per il modello. Ci√≤ semplifica il modello e accelera il funzionamento del modello, migliorando in definitiva le prestazioni del modello.\n",
    "\n",
    "3. **Model Interpretability**:\n",
    "L'importanza delle funzionalit√† √® utile anche per interpretare e comunicare il modello ad altre parti interessate. Calcolando i punteggi per ciascuna funzionalit√†, puoi determinare quali funzionalit√† attribuiscono maggiormente il potere predittivo del tuo modello.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola feature importance del modello\n",
    "def calculate_feature_importance(model, X, y):\n",
    "    # Verifica se il modello ha l'attributo 'feature_importances_'\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance = model.feature_importances_\n",
    "    else:\n",
    "        # Altrimenti, utilizza la 'permutation_importance'\n",
    "        results = permutation_importance(model, X, y, scoring='neg_mean_squared_error')\n",
    "        importance = results.importances_mean\n",
    "\n",
    "    # Ritorna un dizionario con le caratteristiche e la loro importanza\n",
    "    feature_importance = dict(zip(FEATURES, importance))\n",
    "    return feature_importance\n",
    "\n",
    "# Calcola l'importanza delle caratteristiche per ogni modello\n",
    "caratteristiche = []\n",
    "for nome, modello in elenco_modelli.items():\n",
    "    caratteristiche.append((nome, calculate_feature_importance(modello, x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importanze = {}\n",
    "\n",
    "for nome_modello, importanza_caratteristiche in caratteristiche:\n",
    "    importanze[nome_modello] = {}\n",
    "\n",
    "    # Stampa il nome del modello\n",
    "    print(f\"\\nModello: {nome_modello}\")\n",
    "    \n",
    "    # Calcola totale delle importanze\n",
    "    totale = sum(abs(imp) for imp in importanza_caratteristiche.values())\n",
    "    \n",
    "    # Stampa le caratteristice e la loro importanza normalizzata\n",
    "    for caratteristica, importanza in importanza_caratteristiche.items():\n",
    "        importanze[nome_modello][caratteristica] = importanza / totale\n",
    "        print(f\"Caratteristica: {caratteristica}, Importanza: {importanza:.2f}, Importanza nortmalizzata: {((importanza / totale)):.2f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizziamo graficamente i risultati di Feature importance: qui la visualizzazione pi√π appropriata √® sicuramente il barchart multiplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea i ticks per l'asse x\n",
    "xticks = np.arange(N_FEATURES)\n",
    "\n",
    "# Crea un grafico a barre con dimensioni personalizzate\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# Calcola la larghezza delle barre in base al numero di modelli\n",
    "BAR_WIDTH = 0.6\n",
    "bar_width = BAR_WIDTH / len(importanze)\n",
    "\n",
    "# Crea le barre per l'importanza delle caratteristiche per ogni modello di un certo colore\n",
    "for i, (nome, importanza) in enumerate(importanze.items()):\n",
    "    ax.bar(xticks - BAR_WIDTH/2 + i * bar_width, list(importanza.values()), bar_width, label=nome, color=COLORS[i % len(COLORS)])\n",
    "\n",
    "# Aggiungi le etichette, il titolo e la legenda\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Importance')\n",
    "ax.set_title('Feature Importance for All Models')\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels(FEATURES, rotation=90)\n",
    "ax.legend()\n",
    "\n",
    "# Mostra il grafico\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nota subito come tutti i modelli utilizzino l'attributo **rcount** in maniera determinante e gli altri attributi in misura comparativamente molto minore.\n",
    "\n",
    "Questo fatto si poteva gi√† intravedere con la visualizzazione dell'albero decisionale, il quale utilizzava il suddetto attributo per diversi split iniziali."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üçã‚Äçüü©Il metodo **LIME** (Local Interpretable Model-agnostic Explanations) √® una tecnica di trasparenza algoritmica che spiega le predizioni di modelli complessi **su singole istanze** tramite modelli interpretabili e locali.  \n",
    "\n",
    "Quindi si concentra su spiegare singole predizioni piuttosto che modelli globali. Cerca di rispondere alla domanda: ‚Äú_Perch√© il modello ha fatto questa specifica previsione per un dato input_?‚Äù\n",
    "\n",
    "**LIME** perturba i dati di input generando un set di dati simili e valuta le predizioni del modello su questi dati perturbati. Successivamente, costruisce un modello interpretabile (ad esempio, un regressore lineare) con cui effettua le predizioni sui record nell'area locale intorno all'istanza di interesse. Questo approccio permette di comprendere come il modello complesso prende decisioni per specifici campioni mediante l'approssimazione con un modello pi√π semplice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversione di x_train e x_test in array numpy\n",
    "X_train_np = x_train.numpy()\n",
    "X_test_np = x_test.numpy()\n",
    "\n",
    "# Creazione di un oggetto LimeTabularExplainer\n",
    "explainer = LimeTabularExplainer(\n",
    "    X_train_np,\n",
    "    feature_names=FEATURES,\n",
    "    class_names=['target'],\n",
    "    mode='regression'\n",
    ")\n",
    "\n",
    "# Creazione di una figura con un subplot per ogni modello\n",
    "fig, axs = plt.subplots(len(elenco_modelli), 1, figsize=(12, 8*len(elenco_modelli)), squeeze=False)\n",
    "\n",
    "# Seleziona un'istanza casuale\n",
    "index_selected = random.randint(0, len(X_test_np)-1)\n",
    "instance = X_test_np[index_selected]\n",
    "print(f\"Valore reale: {y_test[index_selected].numpy()}\")\n",
    "\n",
    "# Per ogni modello dell'istanza selezionata con LIME\n",
    "for i, (model_name, model) in enumerate(elenco_modelli.items()):\n",
    "\n",
    "    explanation = explainer.explain_instance(\n",
    "        instance, \n",
    "        lambda x: model.predict(tf.convert_to_tensor(x.reshape(-1, x_train.shape[1]))),\n",
    "        num_features=N_FEATURES\n",
    "    )\n",
    "    \n",
    "    # Estrazione degli attributi e delle importanze\n",
    "    attributi, importances = zip(*explanation.as_list())\n",
    "\n",
    "    colors = ['blue' if imp < 0 else 'orange' for imp in importances]\n",
    "    \n",
    "    # Creazione di un barplot per le importanze\n",
    "    ax = axs[i, 0]\n",
    "    ax.barh(range(N_FEATURES), importances, color=colors)\n",
    "    ax.set_yticks(range(N_FEATURES))\n",
    "    ax.set_yticklabels(attributi)\n",
    "    ax.set_xlabel('Attribute Importance')\n",
    "    \n",
    "    # Calcolo della previsione del modello sull'istanza selezionata\n",
    "    predicted_value = model.predict(tf.convert_to_tensor(instance.reshape(1, -1)))\n",
    "\n",
    "    predicted_value_str = ', '.join(f'{v:.4f}' for v in np.atleast_1d(predicted_value.flatten()))\n",
    "    \n",
    "    # Impostazione del titolo del subplot\n",
    "    ax.set_title(f'Model: {model_name} - Predicted Value: {predicted_value_str}')\n",
    "    ax.set_facecolor('white')\n",
    "    \n",
    "    # Calcolo della somma delle importanze positive e negative\n",
    "    positive_sum = sum(imp for imp in importances if imp > 0)\n",
    "    negative_sum = abs(sum(imp for imp in importances if imp < 0))\n",
    "    \n",
    "    # Aggiunta di una leggenda con le somme delle importanze\n",
    "    ax.legend(\n",
    "        [plt.Rectangle((0,0),1,1, color=c) for c in ['blue', 'orange']],\n",
    "        [f'Negative (Sum: {negative_sum:.2f})', f'Positive (Sum: {positive_sum:.2f})'],\n",
    "        title='Influence', loc='lower right'\n",
    "    )\n",
    "\n",
    "# Visualizzazione del grafico\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grafo di Architettura della KAN\n",
    "üï∏Ô∏èQuesto approccio, gi√† utilizzato dagli autori dell'articolo introduttivo alle KAN, permette di visualizzare i grafici delle funzioni spline ai **nodi** della rete.\n",
    "\n",
    "Si ricorda che al nodo $n$ della rete corrisponde la funzione $$l_n = \\sum_{i_{0}=1}^{units_{l-1}}{\\phi_{n,i_0}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definizione della funzione 'get_all_splines_from_model' che estrae tutte le spline da un modello\n",
    "def get_all_splines_from_model(model):\n",
    "    all_splines = []\n",
    "\n",
    "    # Estrae splines di ogni KAN\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, DenseKAN):\n",
    "            splines = layer.get_spline_list()\n",
    "            all_splines.append(splines)\n",
    "            \n",
    "    return all_splines\n",
    "\n",
    "# Definizione della funzione 'create_kan_tree_from_model' che crea un grafo da un modello\n",
    "def create_kan_tree_from_model(model, features):\n",
    "    # Creazione di un grafo diretto\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Aggiunta dei nodi di input al grafo\n",
    "    for i, feature in enumerate(features):\n",
    "        G.add_node(feature, layer=0, pos=(i / (len(features) - 1) if len(features) > 1 else 0.5, 0))\n",
    "    \n",
    "    # Aggiunta dei layer nascosti e di output al grafo\n",
    "    total_layers = len(model.layers)\n",
    "    for layer_idx, layer in enumerate(model.layers):\n",
    "        units = layer.units\n",
    "        for j in range(units):\n",
    "            x_pos = j / (units - 1) if units > 1 else 0.5\n",
    "            G.add_node(f\"h{layer_idx+1}_{j}\", layer=layer_idx+1, pos=(x_pos, -(layer_idx+1) / (total_layers+1)))\n",
    "        \n",
    "        prev_layer_units = len(features) if layer_idx == 0 else model.layers[layer_idx-1].units\n",
    "        for i in range(prev_layer_units):\n",
    "            for j in range(units):\n",
    "                if layer_idx == 0:\n",
    "                    G.add_edge(features[i], f\"h{layer_idx+1}_{j}\")\n",
    "                else:\n",
    "                    G.add_edge(f\"h{layer_idx}_{i}\", f\"h{layer_idx+1}_{j}\")\n",
    "\n",
    "    return G\n",
    "\n",
    "# Definizione della funzione 'draw_merged_edges' che disegna gli archi e le spline nel grafo\n",
    "def draw_merged_edges(G, pos, ax, kan, edge_color='gray'):\n",
    "    # Inizializzazione di una lista vuota per contenere i punti di fusione\n",
    "    merge_points = []\n",
    "    # Per ogni nodo nel grafo\n",
    "    for target in G.nodes():\n",
    "        # Ottiene la lista degli archi entranti nel nodo\n",
    "        in_edges = list(G.in_edges(target))\n",
    "        # Se ci sono pi√π di un arco entrante\n",
    "        if len(in_edges) > 1:\n",
    "            # Calcola la posizione del punto di fusione\n",
    "            target_pos = np.array(pos[target])\n",
    "            merge_point = target_pos + np.array([0, (pos[in_edges[0][0]][1] - target_pos[1]) / 4])\n",
    "            # Aggiunge il punto di fusione alla lista dei punti di fusione\n",
    "            merge_points.append(merge_point)\n",
    "            \n",
    "            # Per ogni arco entrante\n",
    "            for source, _ in in_edges:\n",
    "                # Calcola la posizione di partenza dell'arco\n",
    "                start = np.array(pos[source])\n",
    "                # Disegna l'arco dal punto di partenza al punto di fusione\n",
    "                ax.annotate(\"\", xy=merge_point, xytext=start, arrowprops=dict(arrowstyle=\"-\", color=edge_color, connectionstyle=\"arc3,rad=0.0\"), zorder=1)\n",
    "            \n",
    "            # Disegna un arco dal punto di fusione al nodo\n",
    "            ax.annotate(\"\", xy=target_pos, xytext=merge_point, arrowprops=dict(arrowstyle=\"-\", color=edge_color), zorder=1)\n",
    "    \n",
    "    # Ottiene tutte le spline dal modello\n",
    "    spline_collection = get_all_splines_from_model(kan)\n",
    "    sum_of_spline = []\n",
    "    knots_list = []\n",
    "\n",
    "    # Prende le liste delle spline di cui fare la somma\n",
    "    for spline_model in spline_collection:\n",
    "        for i in range(len(spline_model[0])):\n",
    "            knots_list.append(spline_model[0][0].t)\n",
    "            list_splines = [spline_model[j][i] for j in range(len(spline_model))]\n",
    "            sum_of_spline.append(list_splines)\n",
    "\n",
    "    square_size = 0.06  # Dimensione fissa del quadrato\n",
    "    # Per ogni punto di fusione\n",
    "    for i, point in enumerate(merge_points):\n",
    "        # Se c'√® una spline corrispondente al punto di fusione\n",
    "        if i < len(sum_of_spline):\n",
    "            # Disegna un rettagolo attorno al punto di fusione\n",
    "            square = plt.Rectangle((point[0] - square_size/2, point[1] - square_size/4), square_size, square_size / 2, fill=True, facecolor='white', edgecolor='black', linewidth=2, zorder=2)\n",
    "            ax.add_patch(square)\n",
    "            \n",
    "            # Calcola la somma delle spline entranti in un nodo\n",
    "            spl = sum_of_spline[i]\n",
    "            xx = np.linspace(knots_list[i][0, 0], knots_list[i][0, -1], 100) \n",
    "            yy = 0\n",
    "            for s in spl:\n",
    "                yy += s(xx)\n",
    "            \n",
    "            # Normalizza i valori della spline per adattarsi al quadrato\n",
    "            xx_norm = (xx - np.min(xx)) / (np.max(xx) - np.min(xx)) * square_size + (point[0] - square_size/2)\n",
    "            yy_norm = (yy - np.min(yy)) / (np.max(yy) - np.min(yy)) * (square_size / 2) + (point[1] - square_size / 4)\n",
    "            \n",
    "            # Disegna la spline normalizzata\n",
    "            ax.plot(xx_norm, yy_norm, color='#FF0000', lw=2, alpha=0.7, zorder=3)\n",
    "    \n",
    "    return merge_points\n",
    "\n",
    "\n",
    "# Crea un grafo dal modello\n",
    "G = create_kan_tree_from_model(kan, FEATURES)\n",
    "\n",
    "# Imposta il grafico\n",
    "fig, ax = plt.subplots(figsize=(20, 16))\n",
    "pos = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "# Disegna gli archi e i quadrati con le spline nel grafo\n",
    "merge_points = draw_merged_edges(G, pos, ax, kan)\n",
    "\n",
    "# Disegna i nodi\n",
    "node_collection = nx.draw_networkx_nodes(G, pos, node_size=700, node_color='purple', ax=ax)\n",
    "node_collection.set_zorder(4)  # Imposta l'ordine di sovrapposizione per i nodi\n",
    "\n",
    "# Aggiunge le etichette\n",
    "label_pos = {node: (x, y + 0.02) for node, (x, y) in pos.items()}\n",
    "labels = {node: node if G.nodes[node]['layer'] == 0 else \"\" for node in G.nodes()}\n",
    "label_collection = nx.draw_networkx_labels(G, label_pos, labels, font_size=8, ax=ax)\n",
    "\n",
    "# Aggiunge l'etichetta \"length_of_stay\" sotto il nodo pi√π basso\n",
    "lowest_node = min(pos.items(), key=lambda x: x[1][1])[0]\n",
    "ax.text(pos[lowest_node][0], pos[lowest_node][1] - 0.015, \"length_of_stay\", fontsize=8, ha='center')\n",
    "\n",
    "# Imposta il titolo del grafico\n",
    "plt.title(\"Grafico rappresentazione KAN\")\n",
    "# Nasconde gli assi\n",
    "plt.axis('off')\n",
    "# Ridimensiona il layout del grafico\n",
    "plt.tight_layout()\n",
    "# Mostra il grafico\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
